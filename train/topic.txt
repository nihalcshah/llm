My topic discusses the possible negative ethical effects of dataset biases developed through Natural Language Processing (NLP) machine learning models for use in the general public. Machine Learning is a field so heavily centered around data that it can be very difficult to ignore ethical biases. The difficulty that this presents is that common biases in race and gender can be continuously perpetuated en-masse, often undetected until put in application and observed to a larger scale. Now, text generation, as we know, has managed to mitigate discussion or even analysis of race. Furthermore, many improved tensors of text analysis continue to mitigate the discussions of biases. However, this isn’t the case for new and further developing large-language-models. This has been successful in some instances. GPT-4, for example, still has issues mitigating racial biases in medical prescriptions. There are a wide variety of solutions present and available to NLP developers and AI/ML engineers to help mitigate these long form biases. First of all, direct biases can be removed by dataset filtration and balanced. One possible technique is to perform sentiment neutralization, or the process of filtering information and editing text data till it has zeroed-sentiment score, or doesn’t have any strong words towards a single group. However, this only solves part of the problem. The second half of the solution would be orienting natural language frameworks to ignore larger trends in works that perpetuate biases. For example if many articles show that people of Indian origin are more prone to needing insulin deficits, we do not want this to be reflected in outputs as a possible diagnosis. Not only is this biased, but could possibly be problematic to diabetes. There are a few methods of these, chief amongst them being Hugging Face’s bias detection module and IBM’s AI360 library. Natural Language Processing (NLP) and Large Language Models (LLMs) are two very popular and emerging pieces of technology that have dominated the media and use, especially in our daily lives. Natural Language Processing is a field of AI that works with communication and language generation and analysis through computers. Large Language Models are networks based on the library “transformers” by Google that are trained on billions of words and articles on the internet to create a large amalgamation of text data for generation. My audience has probably been aware of the social impact of biases on certain races. In fact, many of the biases discussed, especially those regarding race or gender, may be common knowledge. The medicine-based detection discussion may be new, as well as the idea of large language models being able to perpetuate stereotypes. A key fact to note is that the type of text-generation model that many of us are familiar with is ChatGPT, which has successfully mitigated many previous biases. I will have to frame this problem in more of a future development issue than a current, widespread issue that is affecting everyone. The solution is very much oriented in data cleaning and highly technical. Now, while basic data cleaning and the fact that the data affects the output may be clear to some members of the audience, even this might be a concept that needs to be grasped. The larger issue is that an understanding of basic machine learning methodology and how data can truly skew the output is not common knowledge. I plan to adapt the topic to the audience through the form of LLMs everyone already knows: ChatGPT. Because the topic of bias can be generic or broad to address, it is easier to bring everyone through a point of contact, like a funny anecdote about ChatGPT and its use. In fact, a quote about large language model bias written by GPT-4 would serve to be very effective. Currently, this is a general application method for text generation that is relatively open source, powered by transformers by Google. Majority of the research is spread across However, two of the bias mitigation methods I refer to are owned by Google and IBM respectively. Each method has its own unique licensing and script ability. The key thing to remind myself is that many members of the audience may not be familiar with the inner workings of a machine learning model, much less an LLM. What this means is that there is a large gap to bridge in understanding, so I will have to be tactful in that. Moving in too fast will leave the audience confused and I will lose them. Another consideration to be made is that I must limit my scope to mitigating biases in Large Language Models and Analysis. The solutions for these two are the same and can be simply covered. However, delving too deep into the specific options of the solutions would not fit the constraints of the talk. This means to touch upon it as Nick’s talk did, but not draw it out. As I said, the most difficult thing is going to be ensuring that the talk remains accessible to an audience that may not be familiar with machine learning. Because it’s such a complex topic, the explanation of the basic tech will take time. I would explain that as we continue to use LLMs, the biases these models perpetuate can impact the information we receive and the decisions we make. I’ll start the talk with a brief introduction to LLMs to provide this context, then move on to the problem of biases stated before, then explain some examples of it happening. Then I will discuss the broader strategies and advice from experts, to draw it back into the modern day and current developments in the topic and how we use LLMs. 

